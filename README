#splelchekcer

**splelchekcer** is a spell-checker written in C++. It is based on the
Damerau-Levenshtein distance working on a precompiled radix trie.

# Build

    $ ./configure && make

# Usage
* `TextMiningCompiler` takes a dictionary of words and the frequency of these
words and it returns a compiled trie.
    $ ./TextMiningCompiler /path/to/words.txt /path/to/dict.bin
* `TextMiningApp` takes the compiled trie and corrects the words with a given
maximum distance.
    $ echo "approx 0 test" | ./TextMiningApp /path/to/dict.bin

# Example
    $ ./TexMiningCompiler project/word.txt dict.bin
    $ ./TextMininApp dict.bin
    > approx 0 test
    [{"word":"test","freq":49216987,"distance":0}]
    > approx 1 vnicent
    [{"word":"vincent","freq":16312153,"distance":1},
     {"word":"vicent","freq":20025,"distance":1},
     {"word":"vncent","freq":666,"distance":1}]

# Questions

## Decrivez les choix de design de votre programme

### TextMiningCompiler

The compiler builds a dynamic radix trie given the dictionary. Nodes and edges
are not contiguous in memory, we use pointers to make all the traversals. A
static radix trie is then serialized into a file, we use a representation that
makes all the data contiguous in memory. Edges and nodes are connected using
offsets.

### TextMiningApp


We use the mmap function to load the serialized radix trie so that the kernel will charge only pages we need in memory.
To navigate fast enough through the structure offset are used. We are using more offset than necessary because
we prioritize navigation speed inside the structure.

Two search algorithm are used :
  - One for an estimated search using Damereau-Levenshtein algorithm.
 Knowing the maximal distance we can compute only a part of the diagonal from
 the table used to compute the distance.

  - For the exact search: as soon as one letter is different words do not match
  so we don't need to build a table. This algorithm is way faster.

## Listez l’ensemble des tests effectués sur votre programme
  (en plus des units tests)

A test on serialization is done:
we serialize first, then deserialize the trie and reserialize it again.
Afterwards, a comparison is made between the two serialization. And finally
we test it against the reference to validate it.

## Avez-vous détecté des cas où la correction par distance ne fonctionnait pas
(même avec une distance élevée) ?

Damerau levenshtein as implemented does not take into account
the position of keys on the keyboard. For instance 'qwerty' and 'asdfgh' are
fairly close on the keyboard but are far away using the Damerau distance. It is 
a limitation of the algorithm we use where it is not as efficient as expected considering the input to write words.

## Quelle est la structure de données que vous avez implémentée dans votre
projet, pourquoi ?

We use a radix trie because it is more compact and its usage reduces
 the number of redirections. It speeds up research and uses less memory.


## Proposez un réglage automatique de la distance pour un programme qui prend juste une chaîne de caractères en entrée, donner le processus d’évaluation ainsi que les résultats

We could use a maximum distance which is inversly proportionnal to the word size.
We then adjust it with a coefficient to determine.
That way the user could uniformly make mistakes and would get adapted 
answer whatever the size of the input word. To evaluate results we could try to see
the number of suggestion given the size of the words, which should stay roughly
at the same number. Also some human validation could be done using a small number
of input words covering a wide array of size.

## Comment comptez vous améliorer les performances de votre programme

We must solve alignment issues when serializing the tire using `__attribute__(packed)`.
We should use more padding between edges so that one read would fit better into a cache.
We should use a more compact representation for the trie.

## Que manque-t-il à votre correcteur orthographique pour qu’il soit à l’état
de l’art ?

It misses how to handle position of letters on the keyboard, using grammar
 to more efficiently link words. It also needs function of words and 
their semantics meaning and finally how to handle punctuation.
